{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) es un modelo generativo que permite que conjuntos de observaciones puedan ser explicados por grupos no observados que explican por qué algunas partes de los datos son similares. Por ejemplo, si las observaciones son palabras en documentos, presupone que cada documento es una mezcla de un pequeño número de categorías (también denominados como tópicos) y la aparición de cada palabra en un documento se debe a una de las categorías a las que el documento pertenece. LDA es un ejemplo de modelo de categorías\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de los tickets de dcip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jenkins dejado funcionar</td>\n",
       "      <td>partir horas dejado funcionar sospecha causas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usuario para artefactos artifactory</td>\n",
       "      <td>necesito artifact estamos generando departamen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>permisos carpetas dcip</td>\n",
       "      <td>podriais permisos carpetas eleven paths dcip 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>repo artifactory</td>\n",
       "      <td>repositorio artifactory para guardar imagenes ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               subject  \\\n",
       "0             jenkins dejado funcionar   \n",
       "1  usuario para artefactos artifactory   \n",
       "2               permisos carpetas dcip   \n",
       "3                     repo artifactory   \n",
       "\n",
       "                                             content  \n",
       "0  partir horas dejado funcionar sospecha causas ...  \n",
       "1  necesito artifact estamos generando departamen...  \n",
       "2  podriais permisos carpetas eleven paths dcip 1...  \n",
       "3  repositorio artifactory para guardar imagenes ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import IPython\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/tickets.csv\")\n",
    "\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacemos el preprocesado ##\n",
    " - Quitamos las stop words\n",
    " - Vectorizamos el texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1990)\t1\n",
      "  (0, 794)\t3\n",
      "  (0, 396)\t3\n",
      "  (0, 1440)\t3\n",
      "  (0, 989)\t3\n",
      "  (0, 323)\t3\n",
      "  (0, 876)\t3\n",
      "  (0, 4403)\t3\n",
      "  (0, 2096)\t5\n",
      "  (0, 1393)\t5\n",
      "  (0, 2309)\t3\n",
      "  (0, 3420)\t3\n",
      "  (1, 4658)\t1\n",
      "  (1, 4635)\t1\n",
      "  (1, 4089)\t1\n",
      "  (1, 515)\t3\n",
      "  (1, 1101)\t3\n",
      "  (1, 4767)\t6\n",
      "  (1, 725)\t6\n",
      "  (1, 3802)\t6\n",
      "  (1, 696)\t3\n",
      "  (1, 3999)\t3\n",
      "  (1, 2136)\t3\n",
      "  (1, 4820)\t4\n",
      "  (1, 4736)\t6\n",
      "  :\t:\n",
      "  (1069, 2397)\t2\n",
      "  (1069, 2292)\t2\n",
      "  (1069, 4283)\t2\n",
      "  (1069, 2192)\t3\n",
      "  (1069, 4687)\t2\n",
      "  (1069, 3652)\t2\n",
      "  (1069, 4030)\t2\n",
      "  (1069, 4800)\t2\n",
      "  (1069, 4207)\t2\n",
      "  (1069, 1165)\t2\n",
      "  (1069, 3155)\t3\n",
      "  (1069, 1176)\t2\n",
      "  (1069, 1524)\t2\n",
      "  (1069, 2139)\t2\n",
      "  (1069, 1168)\t3\n",
      "  (1069, 2509)\t3\n",
      "  (1069, 1779)\t4\n",
      "  (1069, 3384)\t3\n",
      "  (1069, 4889)\t5\n",
      "  (1069, 4376)\t2\n",
      "  (1069, 4585)\t2\n",
      "  (1069, 4523)\t2\n",
      "  (1069, 2312)\t2\n",
      "  (1069, 1262)\t3\n",
      "  (1069, 2846)\t2\n"
     ]
    }
   ],
   "source": [
    "#CountVectorizer implements both tokenization and occurrence counting\n",
    "#  in a single class:\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['content'].values)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacamos los tópicos de cada ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=5,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9f386f20469f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda' is not defined"
     ]
    }
   ],
   "source": [
    "lda.components_.shape\n",
    "lda.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "ituser help contacta marcossuarez contraseña ayuda\n",
      "Topic 2:\n",
      "argos splunkdeploy instance amd64 saas splunk\n",
      "Topic 3:\n",
      "imagen android nuevo crear esclavo repositorio\n",
      "Topic 4:\n",
      "java hudson received plugins origin messaging\n",
      "Topic 5:\n",
      "plugin documento aubay antes esclavo estamos\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 6\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caso [950 948 805 ... 445 429 868]\n",
      "\n",
      "Topic #1:\n",
      "hook pdihub jenkins ...\n",
      "mientras concluida validada migracion todos proyectos dcip queremos mantener compilacion ambos entornos dcip nuestros jenkins plataforma pdihub permite añadir webhook tipo repositorio actualmente para compilacion nuestro jenkins esta activado webhook github plugin funciona correctamente dado posible ...\n",
      "\n",
      "Topic #2:\n",
      "jenkins jira plugin funciona ...\n",
      "jira podemos sacar releasenotes proyecto soprpm mediante consultas jira estando usuario autenticado dado queremos utilizar autenticacion basica queriamos utilizar plugin jenkins para generar releasenotes embargo obtienen ningun resultado muestra ningun error pantalla nuestro proyecto ejemplo dcip te ...\n",
      "\n",
      "Topic #3:\n",
      "dcip solicitud instalacion plugin upstream changes ...\n",
      "vendria bien instalar plugin dcip para poder monitorizar cambios proyecto padre dado lugar build pipeline wiki jenkinsci jenkins upstream changes plugin salu2 aubay marcos pindado sebastian aubay spain juan herrera centro oficina parque tecnologico boecillo valladolid aubay aubay antes imprimir pien ...\n",
      "\n",
      "Topic #4:\n",
      "jenkins dcip ejecutar script después build ...\n",
      "esto estetico menor ocasiones tenemos crear jobs separados debido pueden ejecutar scripts despues fase build podriamos ahorrarnos existiera dicha posibilidad cuestion seria posible instalar algo parecido plugin wiki jenkinsci jenkins postbuildscript plugin posible seguimos hasta salu2 aubay marcos p ...\n",
      "\n",
      "Topic #5:\n",
      "migracion dcip ...\n",
      "estamos analizando impacto tendria migracion nuevo entorno dcip nuestro actual ciclo integracion continua tenemos algunas dudas quedan claras tras consultar wiki seria posible alguno vosotros juntase nosotros para resolverlas salu2 antes imprimir piensa medio confidencialidad documento anexo mismo p ...\n",
      "\n",
      "Topic #6:\n",
      "dcip pending waiting next available executor slaveehealwin ...\n",
      "tras tres ejecuciones jobs prueba esclavo slaveehealwin funciona ninguno mostrando error pendingâ waiting next available executor slaveehealwin tendra problemas genericos encolamiento venido ocurriendo dcip dcip ips219 prueba salu2 aubay marcos pindado sebastian aubay spain juan herrera centro ofici ...\n",
      "\n",
      "Topic #7:\n",
      "importante incrementar espacio disco cimcamulti02 ...\n",
      "solicitado varias ocasiones ampliacion disco maquina cimcamulti02 acaba ampliar 10gb pero suficiente explico situacion actualmente utilizan maquinas integracion continua estamos proceso migracion jobs jenkins dcip ademas estas maquinas tienen jobs para generar entregas software œcriticasâ para nosot ...\n",
      "\n",
      "Topic #8:\n",
      "permisos creacion jobs dcip ...\n",
      "hemos visto creo permisos hora crear jobs dcip principio usuario ips219 tiene permisos para todo dentro proyecto telehealth image002 01d190f4 527f27e0 pero hora crear parte proyecto error image003 01d190f4 527f27e0 intentar copiar proyecto ejemplo muestra error image004 01d190f4 527f27e0 salu2 antes ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mytopic = X_topics[:, 4].argsort()[::-1]\n",
    "print (\"Caso\", mytopic)\n",
    "for iter_idx, movie_idx in enumerate(mytopic[:8]):\n",
    "    print('\\nTopic #%d:' % (iter_idx + 1))\n",
    "    print(df['subject'][movie_idx][:300], '...')\n",
    "    print(df['content'][movie_idx][:300], '...')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.217s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.211s.\n",
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=5000 and n_features=1000...\n",
      "done in 0.139s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: borrada maquinas sido queue action creator amaris listas epgmad9 resuelto borrar resolucion comenzara pondra contigo pronto volvera respondiendo posible borrado\n",
      "Topic #1: docker server team access usage possible support deployment configured work client export issues remember password security environment information version host\n",
      "Topic #2: dcip para jenkins acceso proyecto usuario maquina esta jobs resuelto github permisos posible sido artifactory pdihub pero esclavo mismo muchas\n",
      "Topic #3: ituser contacta help attached inform host command tefdigital returned basecamp citrix backups ones lync puppet necesitas jira best ayuda connection\n",
      "Topic #4: argos splunkdeploy instance saas access information splunk openstack finish enjoy termination launch endpoint description confirmation created dear splunkterminate file credentials\n",
      "Topic #5: received messaging server iplanet built hotfix prodepgdcipm01 esmtp sbrightmailg02 smtp utf8 dhb26 javamail root murano boundary sbrightmailg01 jenkins contenttype returnpath\n",
      "Topic #6: documento aubay anejo piensa imprimir medio mismo antes spain confidencialidad virus anexo confidencial caracter dirigida supuesto error escrita distribucion contener\n",
      "Topic #7: moderator dcipusers approval held awaits posting nonmember membersonly visit cancel decision receive post reason review notification official notifier like report\n",
      "Topic #8: java hudson plugins cligitapiimpl jenkinsci model gitclient remoting threadpoolexecutor gitscm util userrequest origin timeout method remote perform refs concurrent abstractbuild\n",
      "Topic #9: actualizacion programado official notifier enviando podeis nosotros report version habitual para duda poner esta proximo pdihub aclaracion usuarios tareas sobre\n",
      "\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=5000 and n_features=1000...\n",
      "done in 1.299s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: sido queue listas resolucion pronto creator pondra posible volvera action servicio contacto resuelto status satisfaccion contigo mismo comenzara puedes para\n",
      "Topic #1: team dcip check server docker build environments support information usage export host possible wiki regarding variable usual flavour kind deploy\n",
      "Topic #2: para dcip pero usuario xavi esta tenemos muchas proyecto poder podeis jenkins puede tener mismo permisos todos maquina hemos posible\n",
      "Topic #3: wikis virtuales virtual help hello delivered like grupo ituser listas jira videoconferencias delete inform view github openstack lists maquinas gustaria\n",
      "Topic #4: splunk password service splunkdeploy invitado splunkterminate queue file project endpoint saas openstack dear information creator enjoy using problem termination output\n",
      "Topic #5: xjenkinsresult xjenkinsjob smtp utf8 xauditid returnpath esmtp server success javamail delivered iplanet date mimeversion jenkins messageid symantec prodepgdcipm01 mixed hotfix\n",
      "Topic #6: destruccion divulgacion confidencial copia usted prohibida comunique rogamos informacion piensa contener entregarlo entidad otras imprimir personal presente error contenido captura\n",
      "Topic #7: post listas notification queue report reason receive review posting notifier official creator visit nonmember action moderator para held resolucion pronto\n",
      "Topic #8: error workspace listas java version repository contint failed execute plugins hudson connect jenkinsci method remote home fatal estoy jenkins origin\n",
      "Topic #9: report para podeis version official nosotros team queue poner tareas notifier creator enviando duda proximo contacto hemos servicio sobre jenkins\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=5000 and n_features=1000...\n",
      "done in 3.755s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: build dcip origin failed error space timeout pdihub device fetch repository remote left refs workspace repositories artbackups github contint java\n",
      "Topic #1: ituser host attached puppet help contacta jira connection inform tefdigital returned command openstack need lync text wikis artifactory github best\n",
      "Topic #2: artifactory para listas dcip esta report dcipusers moderator juan queue podeis notifier action official creator approval servicio usuarios contacto version\n",
      "Topic #3: usuario acceso dcip para ituser permisos acceder contraseña login ldap necesito usuarios muchas listas proyecto resuelto password pero sido david\n",
      "Topic #4: argos splunkdeploy dcip instance access information openstack saas password splunk invitado service environments endpoint credentials dear project output console default\n",
      "Topic #5: java hudson plugins jenkinsci marcossuarez model jenkins dcip plugin cligitapiimpl marcos network method para suarez remoting gitclient barcelona comunicacion sshagent\n",
      "Topic #6: docker dcip server access team build password version host support queue information possible environment listas client creator action security usage\n",
      "Topic #7: received server prodepgdcipm01 messaging smtp iplanet built hotfix jenkins utf8 esmtp javamail maquinas borrada direccion sbrightmailg02 contenttype boundary build sido\n",
      "Topic #8: aubay documento mismo amd64 setting informacion error dcip anejo confidencial antes imprimir copia medio piensa puede usted comunique personal rogamos\n",
      "Topic #9: dcip para jenkins esta maquina posible sido listas queue jobs proyecto mismo informacion github pero resolucion docker creator action comenzara\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "n_samples = 5000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "data_samples = df.content[:n_samples]\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['content'])\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
